{"componentChunkName":"component---src-templates-blog-detail-js","path":"/blog/python-pandas-google-bigquery","result":{"data":{"contentfulBlog":{"id":"f52e5fd4-1884-5330-90ee-a0c9caf20b23","imageGallery":[{"id":"e440d0da-7e0c-5286-be0e-0cccfa0cf016","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/1Lq8v6SU2cElAFp8KD8Aoo/356cf0a9e7bc4516a69e4469aae19ecd/gbq-step-1.png","fileName":"gbq-step-1.png"},"title":"gbq-step-1","description":""},{"id":"2290d000-2809-594a-87cf-c3c96888c296","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/cTzGLGf0wk0RV7QAIa5J1/fbf739ee665c0c3fc066bd22fbf533aa/gbq-step-2.png","fileName":"gbq-step-2.png"},"title":"gbq-step-2","description":""},{"id":"f2e4f4b0-acea-547f-bc42-359fd2e30e16","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/7tewmCE7oJX0jzFN3E5wKk/f86a63c1b3dd3290298d48847615ccb3/gbq-step-3.png","fileName":"gbq-step-3.png"},"title":"gbq-step-3","description":""},{"id":"c9622b42-20fa-557a-afae-a7c05599f2f0","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/2qeKMLyRhLW0q7hUgdoHgc/8f3306427756afc303e4f77991979d57/gbq-step-4.png","fileName":"gbq-step-4.png"},"title":"gbq-step-4","description":""},{"id":"ab86b88d-d847-5c3d-b22f-d68784e56705","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/8laAGPcPtJaWstNVCI0iQ/e252e3f144870a0743c5da5780aa417a/gbq-step-5-detail.png","fileName":"gbq-step-5-detail.png"},"title":"gbq-step-5-detail","description":""},{"id":"b35de04b-f2db-5e10-bd94-188b1e3a314b","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/IZGItB3zAIT8qPBVj0Uz0/a46b41bb175fd7fb40d3f3894320f8a2/gbq-step-5.png","fileName":"gbq-step-5.png"},"title":"gbq-step-5","description":""},{"id":"ed45c114-acb7-5d32-9b54-6810dce55730","file":{"url":"//images.ctfassets.net/1nc0h0ipk4bl/1joeSzZgWNLbVImOGggaFV/0e89161ef0519856d8ac3a2618776b1e/gbq-step-6.png","fileName":"gbq-step-6.png"},"title":"gbq-step-6","description":""}],"date":"2019-03-30","title":"Using Python and Pandas With Google BigQuery.","subtitle":{"id":"9602f817-9aea-5949-8204-db4cd7fd963b","subtitle":"In this \"how to\", we'll be using Django to query a dataset, format that data using Pandas, then pushing the DataFrame to Google BigQuery as our data warehouse."},"slug":"python-pandas-google-bigquery","mainImage":null,"content":{"id":"e479e968-d2f3-54c9-b807-3228e7251c87","content":"Influen$e relies heavily on data generated through multiple micro-services and API calls, all in real time. Early on, I wanted to set up the platform to use Google BigQuery for data warehousing, allowing us to work with real-time data in smaller chunks.\n\n<section class=\"uk-section uk-section-small\">\n\t<div class=\"content-box uk-container uk-container-small uk-padding-small uk-width-1-2@s\">\n\t\t<h4 class=\"uk-text-center\">Shortcut this</h4>\n\t\t<h5 class=\"uk-text-center\">\n            If you know what you're doing, you can get the code from GitHub.\n        </h5>\n\t\t<div class=\"uk-text-center\">\n\t\t\t<a class=\"uk-button uk-button-large uk-button-secondary\"\n                href=\"https://github.com/MichaelLisboa/python-pandas-bigquery\">\n\t\t\t\tGet it on GitHub\n\t\t\t</a>\n\t\t</div>\n\t</div>\n</section>\n\n#### Trying to make a complicated set up as easy as possible to implement.\nWhen I first started researching how to do this, it all seemed really complicated. I was scouring the web and reading articles, pulling little bits of useful information from many different sources.\n\nIn the end, I came up with a hacked together solution that I refined down to, what I believe, is the simplest execution.\n\n##### What we're going to do.\n\n1. Query data in Django.\n2. Model that data, using Pandas, to a workable DataFrame.\n3. Push the data to Google BigQuery.\n4. Create a Cron job to run nightly pushes.\n\nWhile this is a real-world example, the point of this exercise is to get a basic understanding of the steps required for setting it all up. So, I'll simplify things by only using a single query.\n\n#### Setting up Google BigQuery.\n\nI'm going to skip setting up a Google Cloud project, assuming you've already made it this far. We'll start with getting BigQuery set up.\n\nFirst, you need to go [here and enable the API in your Google Project](https://console.cloud.google.com/flows/enableapi?apiid=bigquery&_ga=2.182927334.-1396141278.1552049696&pli=1&angularJsUrl=%2Fflows%2Fenableapi%3Fapiid%3Dbigquery%26_ga%3D2.182927334.-1396141278.1552049696%26pli%3D1&authuser=1 \"Google BigQuery API\")\n\nGoogle has a really really good walkthrough of [setting up the environment here.](https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui \"BigQuery quick start\")\n\nOnce you have the API setup in your project, an option for BigQuery should be present in your console menu. There you'll see a screen like this:\n\n![gbq-step-1](//images.ctfassets.net/1nc0h0ipk4bl/1Lq8v6SU2cElAFp8KD8Aoo/356cf0a9e7bc4516a69e4469aae19ecd/gbq-step-1.png)\n\n##### Create a DataSet\nClicking on the \"Create Dataset\" button will display this screen:\n\n![gbq-step-2](//images.ctfassets.net/1nc0h0ipk4bl/cTzGLGf0wk0RV7QAIa5J1/fbf739ee665c0c3fc066bd22fbf533aa/gbq-step-2.png)\n\nName your DataSet, in this case I'm calling it `MyDataId`, and choose the zone closest to you. For me, I've selected Singapore. Go ahead and click the Create DataSet button to get your first DataSet set up in BigQuery.\n\nNow that we have a DataSet, we need to add tables to it. To keep things simple, we're going to add only one table. Select your `MyDataId`, and click Create Table to add a new empty table and call it `MyDataTable`. Don't worry about other settings at the moment, an empty table that's editable as text works for our case.\n\n![gbq-step-5](//images.ctfassets.net/1nc0h0ipk4bl/IZGItB3zAIT8qPBVj0Uz0/a46b41bb175fd7fb40d3f3894320f8a2/gbq-step-5.png)\n\nOkay, we now have a DataSet with an empty table set up. We'll be able to reference our table in Python using `MyDataId.MyDataTable`.\n\n##### Let's make our Django View.\n\nGo to your Django app `views.py` and create a new function. I'm calling it `push_bigquery()`. We'll start with creating a queryset for \"MyMembers\":\n```\nfrom django.db.models import Count\nfrom django.db.models.functions import TruncDate\nfrom django.http import HttpResponse\n\nimport pandas as pd\nfrom common.models import MyMembersModel\n\n\ndef push_bigquery(request):\n    members_qs = (\n        MyMembersModel\n        .objects\n        .prefetch_related(\"profile__thing\")\n        .annotate(\n            created_date=TruncDate('profile__thing__timestamp'),\n            count=Count('profile__thing')\n        )\n        .order_by('created_date')\n        .distinct()\n    ).values('created_date', 'name', 'slug', 'count', 'profile__thing')\n    \nmembers_df = pd.DataFrame(members_qs)\n\n```\nWhat we've done here is query our `MyMembers` table and related `Things`, as `values()`, which will return a `dict`-like queryset.\n\nSidenote, I'm also truncating the timstamp using `TruncDate`, which casts the expression to a date rather than using the built-in SQL `truncate` method.\n\nThen we create our Pandas DataFrame from the values with `members_df = pd.DataFrame(members_qs)`\n\n##### Basic data modeling.\n\nWe're going to do some simple modeling to format our DataFrame. Again, we're keeping this simple, I'm assuming you're already knowledgable about formatting data with Pandas.\n\nAdd the following to your function:\n```\npd.to_datetime(df['created_date'])\nmembers_df.set_index(df['created_date'], inplace=True)\n\nmembers_df['count_cumsum'] = members_df['count'].cumsum(axis=0)\n\nmembers_df['label_cumsum'] = members_df.groupby('name')['count'].cumsum()\n\nmembers_df['thing_cumsum'] = (\n    members_df\n    .groupby(df.index)['profile__thing']\n    .cumsum()\n)\n\nmembers_df['frequency_of_thing'] = (\n    members_df['label_cumsum'] / members_df['thing_cumsum'] * 100\n)\n```\nI've started by converting the `created_date` series to datetime and set it as the index.\n\nThen we're getting the cumulative sum of the number of \"things\", and cumulative sum of \"labels\" associated with \"things\". And finally the cumulative sum of \"things\" by each \"member\".\n\nYeah, I know that sounds really confusing...\n\nWe're doing this because we're looking to get the frequency of things `members_df['frequency_of_thing']` by individual members and comparing that to the whole community.\n\nNext we want to format our DataFrame by selecting the Series' we want, filtering out any NaN values and resetting the index:\n```\nmembers_df = members_df[\n    ['name', 'slug', 'label_cumsum', 'thing_cumsum', 'frequency_of_thing']\n]\nmembers_df = members_df[pd.notnull(df['thing_cumsum'])]\nmembers_df.reset_index(inplace=True)\n```\nOkay, our `members_df` is where we want it so we can push it to BigQuery. You'll find this frustratingly simple:\n\n```\nmembers_df.to_gbq(\n    'MyDataId.MyDataTable',\n    project_id='your-gcp-project',\n    if_exists='replace'\n)\n```\nThat's it.\n\nWe're using Pandas `to_gbq` to send our DataFrame to BigQuery.\n\n- `'MyDataId.MyDataTable'` references the DataSet and table we created earlier.\n- `project_id` is obviously the ID of your Google Cloud project.\n- `if_exists` is set to replace the content of the BigQuery table if the table already exists.\n\n##### About if_exists.\n\nIn this case, if the table already exists in BigQuery, we're replacing all of the data. You don't want to do that in the real world. \n\n`if_exists` has a couple of other arguments,\n\n- `fail`, which raises an exception if you try to write to the table, and\n- `append` which is our preferred option in this case, which will append your data to the existing table.\n\n##### One other thing\n\nIn my case, not only do I want to push all my data to BigQuery, I also wanted a subset of that data for fast lookups across different services. To do this I pickeled the last 3 months of my dataset with:\n\n```\nmembers_df.last('3M').to_pickle('members_df.pkl')\n```\nOur final function looks like this:\n```\nfrom django.db.models import Count\nfrom django.db.models.functions import TruncDate\nfrom django.http import HttpResponse\n\nimport pandas as pd\nfrom common.models import MyMembersModel\n\n\ndef push_bigquery(request):\n    members_qs = (\n        MyMembersModel\n        .objects\n        .prefetch_related(\"profile__thing\")\n        .annotate(\n            created_date=TruncDate('profile__thing__timestamp'),\n            count=Count('profile__thing')\n        )\n        .order_by('created_date')\n        .distinct()\n      ).values('created_date', 'name', 'slug', 'count', 'profile__thing')\n\n    members_df = pd.DataFrame(members_qs)\n\n    pd.to_datetime(df['created_date'])\n    members_df.set_index(df['created_date'], inplace=True)\n\n    members_df['count_cumsum'] = members_df['count'].cumsum(axis=0)\n\n    members_df['label_cumsum'] = members_df.groupby('name')['count'].cumsum()\n\n    members_df['thing_cumsum'] = (\n        members_df\n        .groupby(df.index)['profile__thing']\n        .cumsum()\n    )\n\n    members_df['frequency_of_thing'] = (\n        members_df['label_cumsum'] / members_df['thing_cumsum'] * 100\n    )\n\n    members_df = members_df[\n        ['name', 'slug', 'label_cumsum', 'thing_cumsum', 'frequency_of_thing']\n    ]\n    members_df = members_df[pd.notnull(df['thing_cumsum'])]\n    members_df.reset_index(inplace=True)\n\n    members_df.to_gbq(\n        'MyDataId.MyDataTable',\n        project_id='your-gcp-project',\n        if_exists='replace'\n    )\n\n    members_df.last('3M').to_pickle('members_df.pkl')\n\n    return HttpResponse(status=200)\n```\n\nWe could do a bit more optimization with Pandas and Threading, but this is good enough for the purpose of this article.\n\n##### Google Cloud App Engine &amp; Cron jobs\nWe're going to set up a Cron job on Google App Engine to run our BigQuery program nightly.\n\nLet's create a url pattern for our `push_bigquery` view and add it to the app urls.py:\n\n```\n# app/urls.py\n\nfrom django.conf.urls import url\n\nfrom . import views\n\nurlpatterns = [\n\n    ... other url patterns ...\n    \n    url(\n        r'^push-gbq/$',\n        views.push_bigquery\n    ),\n]\n```\nNow you should be able to visit http://localhost:8000/push-gbq/ and the function will run, cerating the DataFrame, pushing it to BigQuery, and pickeling your DataFrame.\n\nTo make sure it works, take a look at BigQuery in your Google Cloud Console. You should see your table update with your data. Also, you should see a new `.pkl` file called \"members_df.pkl\" in your local project root directory.\n\nNext, in your project root directory, create a file called `cron.yaml` and add this:\n\n```\n# root/cron.yaml\n\ncron:\n- description: \"Push GBQ CRON\"\n  url: /push-gbq/\n  schedule: every 24 hours\n  retry_parameters:\n    min_backoff_seconds: 120\n    max_doublings: 5\n```\nThis creates a Cron job for App Engine that will visit your push_bigquery function at `www.your-website/push-gbq/` every 24 hours to push your latest data to BigQuery. Depending on your situation, you'll probably want to change the frequency this runs in your `cron.yaml` file.\n\nPush the Cron job to App Engine with this terminal command:\n\n`gcloud app deploy cron.yaml`\n\n##### Security\n\nMake sure you set up your function to only accept requests with the `x-appengine-cron` header!\n\nNow you can deploy your project `gcloud app deploy` and test your Cron job by clicking the \"Run now\" button on your Cron page in Google Console.\n"}}},"pageContext":{"slug":"python-pandas-google-bigquery"}}}